{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dbc8ea3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T20:08:31.215141Z",
     "start_time": "2022-01-08T20:08:29.576144Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datetime import datetime\n",
    "from utils import Json_Parser\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from dataset_covernet import NuSceneDataset_CoverNet\n",
    "from torch import optim,Tensor,unsqueeze\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.autograd\n",
    "import torch.nn as nn\n",
    "from torchvision.models.resnet import resnet50\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2ca4116",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T20:08:32.702585Z",
     "start_time": "2022-01-08T20:08:31.216142Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "371"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader = DataLoader(NuSceneDataset_CoverNet(train_mode=True, config_file_name='./covernet_config.json', verbose=False), \n",
    "                              batch_size=2, shuffle=True, num_workers=8, prefetch_factor = 2,\n",
    "                              pin_memory = True, persistent_workers=True)\n",
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd5c976c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T20:08:32.733556Z",
     "start_time": "2022-01-08T20:08:32.703586Z"
    }
   },
   "outputs": [],
   "source": [
    "from l5kit.configs import load_config_data\n",
    "cfg = load_config_data(\"./agent_motion_config.yaml\")\n",
    "train_cfg = cfg[\"train_data_loader\"]\n",
    "# 基本参数\n",
    "if cfg[\"train_params\"][\"device\"] == 1:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "epochs = cfg[\"train_params\"][\"epochs\"]\n",
    "latent_dim = cfg[\"model_params\"][\"latent_dim\"]  # LSTM 的单元个数\n",
    "encoder_fc = 64\n",
    "num_layers = cfg[\"model_params\"][\"num_layers\"]\n",
    "bidirectional = cfg[\"model_params\"][\"bidirectional\"]\n",
    "\n",
    "encoder_length = cfg[\"model_params\"][\"history_num_frames\"]\n",
    "decoder_length = cfg[\"model_params\"][\"future_num_frames\"]\n",
    "num_encoder_tokens = 2\n",
    "num_decoder_tokens = 2\n",
    "z_dimension = 32\n",
    "accumulation_steps = 5 # 梯度累积步数\n",
    "\n",
    "num_classes = 3 # 类数\n",
    "modal_fc = latent_dim*(1+bidirectional) \n",
    "thre = 0.175 # yaw_threshold=10°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67c52b4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T20:08:32.749569Z",
     "start_time": "2022-01-08T20:08:32.734556Z"
    }
   },
   "outputs": [],
   "source": [
    "def neg_multi_log_likelihood_batch(\n",
    "    gt: Tensor, pred: Tensor, confidences: Tensor\n",
    ") -> Tensor:\n",
    "    assert len(pred.shape) == 4, f\"expected 3D (MxTxC) array for pred, got {pred.shape}\"\n",
    "    batch_size, num_modes, future_len, num_coords = pred.shape\n",
    "\n",
    "    assert gt.shape == (batch_size, future_len, num_coords), f\"expected 2D (Time x Coords) array for gt, got {gt.shape}\"\n",
    "    assert confidences.shape == (batch_size, num_modes), f\"expected 1D (Modes) array for gt, got {confidences.shape}\"\n",
    "    assert torch.allclose(torch.sum(confidences, dim=1), confidences.new_ones((batch_size,))), \"confidences should sum to 1\"\n",
    "#     assert avails.shape == (batch_size, future_len), f\"expected 1D (Time) array for gt, got {avails.shape}\"\n",
    "    # assert all data are valid\n",
    "    assert torch.isfinite(pred).all(), \"invalid value found in pred\"\n",
    "    assert torch.isfinite(gt).all(), \"invalid value found in gt\"\n",
    "    assert torch.isfinite(confidences).all(), \"invalid value found in confidences\"\n",
    "#     assert torch.isfinite(avails).all(), \"invalid value found in avails\"\n",
    "\n",
    "    # convert to (batch_size, num_modes, future_len, num_coords)\n",
    "    gt = torch.unsqueeze(gt, 1)  # add modes\n",
    "#     avails = avails[:, None, :, None]  # add modes and cords\n",
    "\n",
    "    # error (batch_size, num_modes, future_len)\n",
    "    error = torch.sum(((gt - pred)) ** 2, dim=-1)  # reduce coords and use availability\n",
    "\n",
    "    with np.errstate(divide=\"ignore\"):  # when confidence is 0 log goes to -inf, but we're fine with it\n",
    "        # error (batch_size, num_modes)\n",
    "        error = torch.log(confidences) - 0.5 * torch.sum(error, dim=-1)  # reduce time\n",
    "\n",
    "    # use max aggregator on modes for numerical stability\n",
    "    # error (batch_size, num_modes)\n",
    "    max_value, _ = error.max(dim=1, keepdim=True)  # error are negative at this point, so max() gives the minimum one\n",
    "    error = -torch.log(torch.sum(torch.exp(error - max_value), dim=-1, keepdim=True)) - max_value  # reduce modes\n",
    "    # print(\"error\", error)\n",
    "    return torch.mean(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fadac25e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T20:08:35.227941Z",
     "start_time": "2022-01-08T20:08:32.750570Z"
    }
   },
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CVAE, self).__init__()\n",
    "\n",
    "        # 定义序列编码器\n",
    "        self.encoder = nn.LSTM(\n",
    "            num_encoder_tokens, latent_dim, num_layers=num_layers, bidirectional=bidirectional, batch_first=True)\n",
    "        self.encoder2 = nn.Linear(latent_dim*(1+bidirectional), encoder_fc)\n",
    "#         self.encoder2 = nn.Linear(latent_dim*(1+bidirectional)+modal_fc, encoder_fc)\n",
    "#         self.encoder_mean1 = nn.Linear(latent_dim*(1+bidirectional), 64)\n",
    "        self.encoder_mean2 = nn.Linear(encoder_fc, z_dimension)\n",
    "#         self.encoder_std1 = nn.Linear(latent_dim*(1+bidirectional), 32)\n",
    "        self.encoder_std2 = nn.Linear(encoder_fc, z_dimension)\n",
    "\n",
    "        # 定义序列解码器\n",
    "        self.decoder = nn.LSTM(z_dimension*2, latent_dim, num_layers=num_layers,\n",
    "                               bidirectional=bidirectional, batch_first=True)\n",
    "        self.decoder_fc = nn.Linear(latent_dim*(1+bidirectional), 64)\n",
    "        self.decoder_fc1 = nn.Linear(64, num_decoder_tokens)\n",
    "        self.decoder_fc2 = nn.Linear(64, num_decoder_tokens)\n",
    "        self.decoder_fc3 = nn.Linear(64, num_decoder_tokens)\n",
    "        self.decoder_confi = nn.Linear(6, num_classes)\n",
    "\n",
    "        # 定义图像编码器\n",
    "        # load pre-trained Conv2D model\n",
    "        self.resnet = resnet50(pretrained=True)\n",
    "        # change input channels number to match the rasterizer's output\n",
    "        num_in_channels = 3\n",
    "        self.resnet.conv1 = nn.Conv2d(\n",
    "            num_in_channels,\n",
    "            self.resnet.conv1.out_channels,\n",
    "            kernel_size=self.resnet.conv1.kernel_size,\n",
    "            stride=self.resnet.conv1.stride,\n",
    "            padding=self.resnet.conv1.padding,\n",
    "            bias=False,\n",
    "        )\n",
    "        # change output size to (X, Y) * number of future states\n",
    "        num_targets = z_dimension * cfg[\"model_params\"][\"future_num_frames\"]\n",
    "        self.resnet.fc = nn.Linear(in_features=2048, out_features=512)\n",
    "        self.encoder_mean3 = nn.Linear(512, num_targets)\n",
    "        self.encoder_std3 = nn.Linear(512, num_targets)\n",
    "        \n",
    "        #定义采样器\n",
    "        self.sampler_fc1 = nn.Linear(3,1024)\n",
    "        self.sampler_fc2 = nn.Linear(1024,512)\n",
    "        self.sampler_fc3 = nn.Linear(512,z_dimension * cfg[\"model_params\"][\"future_num_frames\"])\n",
    "        \n",
    "        #定义行为预测\n",
    "        self.modal1 = nn.LSTM(\n",
    "            num_encoder_tokens, latent_dim, num_layers=num_layers, bidirectional=bidirectional, batch_first=True)\n",
    "        self.modal2 = nn.Linear(latent_dim*(1+bidirectional), 3)\n",
    "        \n",
    "    def noise_reparameterize(self, mean, logvar):\n",
    "        eps = torch.randn(mean.shape).to(device)\n",
    "        z = mean + eps * torch.exp(logvar)\n",
    "        return z\n",
    "\n",
    "    def forward(self, data):\n",
    "#         print(data[\"history_positions\"])\n",
    "        inputs1 = data[\"history_positions\"].to(device)\n",
    "        inputs1 = inputs1.float()\n",
    "#         yaw = torch.FloatTensor(data[\"history_yaws\"]).to(device)\n",
    "        if inputs1.dim() == 2:\n",
    "            inputs1 = torch.unsqueeze(inputs1, 0)\n",
    "\n",
    "        h0 = torch.autograd.Variable(torch.randn(\n",
    "            num_layers*(1+bidirectional), inputs1.size()[0], latent_dim)).to(device)\n",
    "        c0 = torch.autograd.Variable(torch.randn(\n",
    "            num_layers*(1+bidirectional), inputs1.size()[0], latent_dim)).to(device)\n",
    "\n",
    "        inputs21 = data[\"image\"].to(device)\n",
    "        inputs21 = inputs21.float()\n",
    "        inputs2 = inputs21.permute(0, 3, 1, 2)\n",
    "        if inputs2.dim() == 3:\n",
    "            inputs2 = torch.unsqueeze(inputs2, 0)\n",
    "\n",
    "        out1, _ = self.encoder(inputs1, (h0, c0))\n",
    "#         out1 = out1[:,-1,:]\n",
    "#         out1 = torch.unsqueeze(out1, 1)\n",
    "#         out1 = out1.expand(out1.size()[0],decoder_length,out1.size()[-1])\n",
    "        out1 = F.relu(self.encoder2(out1), inplace=True)\n",
    "        \n",
    "        out_modal, _ = self.modal1(inputs1, (h0, c0))\n",
    "        out_modal = F.softmax(self.modal2(out_modal[:, -1, :]), dim = -1)\n",
    "\n",
    "#         mean1 = F.relu(self.encoder_mean1(out1), inplace=True)\n",
    "        mean2 = F.relu(self.encoder_mean2(out1), inplace=True)\n",
    "#         logstd1 = F.relu(self.encoder_std1(out1), inplace=True)\n",
    "        logstd2 = F.relu(self.encoder_std2(out1), inplace=True)\n",
    "        # prevent from poster vanish\n",
    "#         logstd2 = torch.abs(logstd2) + 0.6\n",
    "\n",
    "        z1 = self.noise_reparameterize(mean2, logstd2)\n",
    "        z1 = z1[:, -1, :]\n",
    "        z1 = torch.unsqueeze(z1, 1)\n",
    "        z1 = z1.expand(z1.size()[0], decoder_length, z1.size()[-1])\n",
    "\n",
    "        out12 = self.resnet(inputs2)\n",
    "        mean3 = F.relu(self.encoder_mean3(out12), inplace=True)\n",
    "        logstd3 = F.relu(self.encoder_std3(out12), inplace=True)\n",
    "        z2 = self.noise_reparameterize(mean3, logstd3)\n",
    "        z2 = z2.reshape(z1.size())\n",
    "        z = torch.cat([z1, z2], -1)\n",
    "        out2, _ = self.decoder(z)\n",
    "        out2 = F.relu(self.decoder_fc(out2), inplace=True)\n",
    "\n",
    "        out21 = F.relu(self.decoder_fc1(out2), inplace=True)\n",
    "        out22 = F.relu(self.decoder_fc2(out2), inplace=True)\n",
    "        out23 = F.relu(self.decoder_fc3(out2), inplace=True)\n",
    "        confs = torch.cat([out21, out22, out23], dim=-1)\n",
    "        confidences = F.softmax(self.decoder_confi(confs)[:, -1, :], dim=-1)\n",
    "        confidences = F.softmax(confidences * out_modal, dim=-1)\n",
    "        \n",
    "        out21 = torch.unsqueeze(out21, 1)\n",
    "        out22 = torch.unsqueeze(out22, 1)\n",
    "        out23 = torch.unsqueeze(out23, 1)\n",
    "        y_hat = torch.cat([out21, out22, out23], dim=1)\n",
    "\n",
    "        return y_hat, confidences, mean2, logstd2, mean3, logstd3\n",
    "\n",
    "\n",
    "def label_maker(yaw):\n",
    "    yaw = yaw.squeeze()\n",
    "#     print(yaw.size())\n",
    "    ind = torch.argmax(torch.abs(yaw), dim=-1)\n",
    "#     print(ind.size())\n",
    "    yaw = yaw[torch.arange(len(yaw)), ind]\n",
    "    yaw = yaw.unsqueeze(dim=-1)\n",
    "    w = torch.cat([yaw, yaw, yaw], dim=-1)\n",
    "    one = torch.ones_like(yaw)\n",
    "    label1 = torch.tensor([0., 0., 1.]).to(device)\n",
    "    label1 = label1.expand(len(yaw), num_classes)\n",
    "    label2 = torch.tensor([0., 1., 0.]).to(device)\n",
    "    label2 = label2.expand(len(yaw), num_classes)\n",
    "    label3 = torch.tensor([1., 0., 0.]).to(device)\n",
    "    label3 = label3.expand(len(yaw), num_classes)\n",
    "\n",
    "    w = torch.where(yaw >= thre, label1, w)\n",
    "    yaw = torch.where(yaw >= thre, one, yaw)\n",
    "\n",
    "    w = torch.where(yaw <= -thre, label3, w)\n",
    "    yaw = torch.where(yaw <= -thre, one, yaw)\n",
    "\n",
    "    w = torch.where(yaw < thre, label2, w)\n",
    "#     w = torch.LongTensor(w).to(device)\n",
    "    return w\n",
    "\n",
    "\n",
    "def loss_function(y_hat, confidences, data, mean1, std1, mean2, std2):\n",
    "#     y_availabilities = data[\"target_availabilities\"].to(device)\n",
    "#     yaw = data[\"target_positions\"][:,2:].to(device)\n",
    "#     label = label_maker(yaw)\n",
    "    y_true = data[\"target_positions\"].to(device)\n",
    "#     MSE = F.mse_loss(y_hat, y_true, reduction='none')\n",
    "#     MSE = MSE * y_availabilities\n",
    "#     MSE = MSE.mean()\n",
    "#     Cross = F.binary_cross_entropy_with_logits(confidences, label)\n",
    "    NLL = neg_multi_log_likelihood_batch(\n",
    "        y_true, y_hat, confidences)\n",
    "    # 因为var是标准差的自然对数，先求自然对数然后平方转换成方差\n",
    "    var1 = torch.pow(torch.exp(std1), 2)\n",
    "    var2 = torch.pow(torch.exp(std2), 2)\n",
    "    KLD1 = -0.5 * torch.mean(1+torch.log(var1)-torch.pow(mean1, 2)-var1)\n",
    "    KLD1 = torch.max(KLD1,torch.ones_like(KLD1))\n",
    "    KLD2 = -0.5 * torch.mean(1+torch.log(var2)-torch.pow(mean2, 2)-var2)\n",
    "    KLD2 = torch.max(KLD2,torch.ones_like(KLD2))\n",
    "    KLD = KLD1 + KLD2\n",
    "#     print('KLD: ',KLD,' NLL: ',NLL,' Cross: ', Cross)\n",
    "    return NLL, KLD, 0\n",
    "\n",
    "\n",
    "# 创建对象\n",
    "cvae = CVAE().to(device)\n",
    "# vae.load_state_dict(torch.load('./VAE_z2.pth'))\n",
    "cvae_optimizer = torch.optim.Adam(cvae.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "421b9635",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-08T20:09:15.698136Z",
     "start_time": "2022-01-08T20:08:35.228942Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1511.083238305054 loss(avg): 1032.6272854444069:   1%|▎                          | 4/371 [00:17<27:11,  4.45s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2872/3112155274.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0maccumulation_steps\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m                 \u001b[0mcvae_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m                 \u001b[0mcvae_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_to_none\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\stgm\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\stgm\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\stgm\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    113\u001b[0m                         \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m                         \u001b[1;31m# Exponential moving average of gradient values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m                         \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'exp_avg'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m                         \u001b[1;31m# Exponential moving average of squared gradient values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m                         \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'exp_avg_sq'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not cfg['mode']['load_mode']:    \n",
    "    # ==== TRAIN LOOP\n",
    "    losses_avg = []\n",
    "    for epoch in range(epochs):  # 进行多个epoch的训练\n",
    "        tr_it = iter(train_dataloader)\n",
    "        progress_bar = tqdm(range(len(train_dataloader)//1),position=0)\n",
    "        losses_train = []\n",
    "        cvae_optimizer.zero_grad(set_to_none = True)\n",
    "        for i in progress_bar:\n",
    "            try:\n",
    "                data = next(tr_it)\n",
    "            except StopIteration:\n",
    "                tr_it = iter(train_dataloader)\n",
    "                data = next(tr_it)\n",
    "            cvae.train() # 设置为训练模式\n",
    "            torch.set_grad_enabled(True)\n",
    "#             print(data[\"image\"].size(),data[\"history_positions\"].size(),data[\"target_positions\"].size())\n",
    "            y_hat, confidences, mean1, std1, mean2, std2 = cvae(data)  # 输入\n",
    "            if cfg[\"train_params\"][\"device\"] == 1:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    NLL,KLD,Cross = loss_function(y_hat, confidences, data, mean1, std1, mean2, std2)\n",
    "                    loss = NLL + (25)*KLD + 20*Cross\n",
    "                    if i + 1>= len(train_dataloader)//1:\n",
    "                        print(NLL,KLD,Cross)\n",
    "            else:\n",
    "                NLL,KLD,Cross = loss_function(y_hat, confidences, data, mean1, std1, mean2, std2)\n",
    "                loss = NLL + (25)*KLD + 20*Cross\n",
    "                if i + 1>= len(train_dataloader)//1:\n",
    "                    print(NLL,KLD,Cross)\n",
    "\n",
    "            # Backward pass\n",
    "            # 梯度累积模式\n",
    "            loss = loss / accumulation_steps\n",
    "            loss.backward() \n",
    "            if (i+1) % accumulation_steps == 0:\n",
    "                cvae_optimizer.step()\n",
    "                cvae_optimizer.zero_grad(set_to_none = True)\n",
    "\n",
    "            # 无梯度累积模式\n",
    "    #         cvae_optimizer.zero_grad(set_to_none = True)\n",
    "    #         loss.backward()\n",
    "    #         cvae_optimizer.step()\n",
    "            losses_train.append(loss.item())\n",
    "            progress_bar.set_description(f\"loss: {loss.item()} loss(avg): {np.mean(losses_train)}\")\n",
    "        losses_avg.append(np.mean(losses_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0335e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stgm",
   "language": "python",
   "name": "stgm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
